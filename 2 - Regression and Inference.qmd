---
title: "2 - Regression and Inference"
author: "Austin Mangelson"
format: html
editor: visual
---

## [The Effect](https://theeffectbook.net/ch-DescribingVariables.html)

**Chapter 3 - Describing Variables**

[Variable Types]{.underline}

continuous - can take any value

count - can't be negative, can't be fractional

ordinal - take more or less (ex. low, medium, high, or elementary, middle, high school, and college

categorical - belong in a "bucket"\
binary categorical (ex. yes or no)

qualitative - catch-all for everything else

[Distribution]{.underline}

bar graphs, histograms, density plots, etc.

[Summarizing distributions]{.underline}

mean, median, mode, etc.

mean produces representative value, median produces representative observation

[Variance]{.underline}

1.  find the mean (ex. 2, 5, 5, 6, mean = 4.5)
2.  subtract mean from value - variation around the mean (ex. -2.5, 0.5, 0.5, 1.5)
3.  square each value (6.25, 0.25, 0.25, 2.25)
4.  add them up! (9)
5.  divide by number of observations minus 1. In our example, our sample variance is 9/(4-1) = 3

the bigger the **variance**, the more variation there is in that variable.

**IQR** (inter quartile range) = difference between 75th and 25th percentiles. It covers half your sample closest to the median. Not effected by big tail observations

**90/10 ratio** = 90th percentile divided by 10th. Tells you how different the top and bottom of distribution are

[Skew]{.underline}

right skew = long right tail

left skew = long left tail

[Notation]{.underline}

Î¼ = mean

Ïƒ = standard deviation

Ï = correlation

Î² = regressionÂ coefficients

Îµ = â€œerror termsâ€

# [Regression and Inference](https://evalsp24.classes.andrewheiss.com/content/02-content.html)

1.  Drawing lines
2.  lines Greek letters, and regression
3.  null worlds and statistical significance

**Drawing lines**

y = outcome variable (response variable, dependent variable, etc.)\
the thing you want to explain or predict

x = explanatory variable (predictor or independent variable)\
thing you use to predict or explain

Examples:

| X                                                                   | Y                                |
|---------------------------------------------------------------------|----------------------------------|
| effect of smoking                                                   | on lung cancer                   |
| does taking more AP classes                                         | improve college grades           |
| effect of negative media coverage, revolutions, and economic growth | on genocide                      |
| viewing history                                                     | to predict future shows to watch |

2 purposes of regression:

1.  PREDICTION, forecasts the future (the focus is on Y) - goal is to add effects
2.  EXPLANATION, explain effect of x on y (focus is on X) - goal is to isolate one effect

**How?**

1.  plot X and Y
2.  draw a line that approximates the relationship (and that would plausibly work for data not in the sample)
3.  find mathy parts of the line
4.  interpret the math

residual errors - how far away from the line are we?

OLS (ordinary least squares) Regression = minimizing the distance between all points and the best line of fit

**Lines, Greek, and regression**

y = mx + b

stats use a sample to make inferences about a population

[GREEK = the *truth*]{.underline}

Î² = the truth,

BÌ‚ = our estimation of the truth

[LATIN = actual data]{.underline}

X = actual data

xÌ„ = calculations from sample

Data â€“\> calculation â€“\> estimate â€“\> truth

**In stats, *y = mx + b* becomes y\^ = B0 + B 1x1 + e (*residual errors*)**

```{r}
name_of_model <- lm(<Y> ~ <X>, data = <DATA>)

summary(name_of_model) #see model details

library(broom)

#convert model results to a data frame for plotting
tidy(name_of_model)

# convert model diagnostics to a data frame
glance(name_of_model)
```

```{r}

library(tidyverse)

cookies_data <- tibble(
  happiness = c(0.5, 2, 1, 2.5, 3, 1.5, 2, 2.5, 2, 3),
  cookies = 1:10
)

happiness_model <- lm(happiness ~ cookies, data = cookies_data)

tidy(happiness_model, conf.int = TRUE)

glance(happiness_model)
```

[**Template for single variables:**]{.underline}

**A one unit increase in X is *associated* with a Î²~1~ increase (or decrease) in Y, on average**

Ë†happiness = Î²0 + Î²1cookies + Îµ

Ë†happiness = 1.1 + 0.16Ã—cookies + Îµ

[**Template for multiple variables:**]{.underline}

y = Î²0 + Î²1x1 + Î²2x2 + â‹¯ + Î²nxn + Îµ

```{r}
#~ = "as explained by"
car_model <- lm(hwy ~ displ + cyl + drv,
                data = mpg)

tidy(car_model, conf.int = TRUE)
```

Categorical variables always have a "base case" (ex. 4 wheel drive above)

[**Template for continuous variables:**]{.underline}

***Holding everything else constant*****, a one unit increase in X is *associated* with a Î²~n~ increase (or decrease) in Y, on average**

Ë†hwy=Â 33.1+(âˆ’1.12)Ã—displ+(âˆ’1.45)Ã—cylÂ +(5.04)Ã—drv:f+(4.89)Ã—drv:r+Îµ

*"On average, a one unit increase in cylinders is associated with 1.45 lower highway MPG, holding everything else constant"*

[**Template for categorical variables:**]{.underline}

***Holding everything else constant*****, Y is Î²~n~ units larger (or smaller) in X~n~, compared to X~omitted~, on average**

Ë†hwy=Â 33.1+(âˆ’1.12)Ã—displ+(âˆ’1.45)Ã—cylÂ +(5.04)Ã—drv:f+(4.89)Ã—drv:r+Îµ

*"On average, front-wheel drive cars have 5.04 higher highway MPG than 4-wheel-drive cars, holding everything else constant"*

lnYi=Î²0+Î²1Pi+Î²2Ai+Î²3SATi+Î²4PIi+ei

```{r}
#economists
lnYi=Î±+Î²Pi+Î³Ai+Î´1SATi+Î´2PIi+ei

#statiticians
lnYi=Î²0+Î²1Pi+Î²2Ai+Î²3SATi+Î²4PIi+ei

#R programming
lm(log(income) ~ private + group_a + sat + parental_income, 
   data = income_data)
```

**Null worlds and statistical significance**

Xâ†’Â¯Xâ†’\^Î¼ðŸ¤ž hopefully ðŸ¤žâˆ’âˆ’âˆ’âˆ’âˆ’âˆ’âˆ’âˆ’âˆ’âˆ’â†’Î¼\
data â€“\> calculation â€“\> estimate â€“\> truth

[Are action movies rates higher than comedies?]{.underline}

```{r}
install.packages("ggplot2movies")
library(ggplot2movies)

head(movies)

```

p-value = probability of seeing something in a world where the effect is 0

If p \< 0.05, there's a good chance the estimate is not 0 and is "real"\
If p \> 0.05, we can't say anything (*doesn't mean there's no effect! Just that we can't tell if there is*)

**You can find a p-value for any Greek letter estimate, like Î² from a regression\
**(in a null world, slope (B) would be 0)

```{r}
tidy(car_model, conf.int = TRUE)
```
